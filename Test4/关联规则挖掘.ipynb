{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T09:39:22.594360800Z",
     "start_time": "2025-05-15T09:39:22.527855600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('Market_Basket_Optimisation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# 创建一个空列表records，用于存储每个事务中的商品\n",
    "records = []\n",
    "# 遍历数据集中的每一行\n",
    "records = data.apply(lambda row: row.dropna().astype(str).tolist(), axis=1).tolist()\n",
    "\n",
    "\n",
    "# 使用TransactionEncoder将事务数据转换为适合Apriori算法处理的布尔矩阵格式\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T09:39:24.442221100Z",
     "start_time": "2025-05-15T09:39:22.570146200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori频繁项集：\n",
      "     support                    itemsets\n",
      "0   0.087200                   (burgers)\n",
      "1   0.081067                      (cake)\n",
      "2   0.060000                   (chicken)\n",
      "3   0.163867                 (chocolate)\n",
      "4   0.080400                   (cookies)\n",
      "5   0.051067               (cooking oil)\n",
      "6   0.179733                      (eggs)\n",
      "7   0.079333                  (escalope)\n",
      "8   0.170933              (french fries)\n",
      "9   0.063200           (frozen smoothie)\n",
      "10  0.095333         (frozen vegetables)\n",
      "11  0.052400             (grated cheese)\n",
      "12  0.132000                 (green tea)\n",
      "13  0.098267               (ground beef)\n",
      "14  0.076400            (low fat yogurt)\n",
      "15  0.129600                      (milk)\n",
      "16  0.238267             (mineral water)\n",
      "17  0.065733                 (olive oil)\n",
      "18  0.095067                  (pancakes)\n",
      "19  0.071333                    (shrimp)\n",
      "20  0.050533                      (soup)\n",
      "21  0.174133                 (spaghetti)\n",
      "22  0.068400                  (tomatoes)\n",
      "23  0.062533                    (turkey)\n",
      "24  0.058533          (whole wheat rice)\n",
      "25  0.052667  (chocolate, mineral water)\n",
      "26  0.050933       (mineral water, eggs)\n",
      "27  0.059733  (mineral water, spaghetti)\n",
      "\n",
      "Apriori关联规则：\n",
      "Empty DataFrame\n",
      "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, representativity, leverage, conviction, zhangs_metric, jaccard, certainty, kulczynski]\n",
      "Index: []\n",
      "\n",
      "Apriori高提升度关联规则：\n",
      "Empty DataFrame\n",
      "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, representativity, leverage, conviction, zhangs_metric, jaccard, certainty, kulczynski]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Apriori算法生成频繁项集\n",
    "# 使用apriori函数，设置最小支持度为0.05，use_colnames=True表示使用DataFrame的列名作为项集的名称\n",
    "frequent_itemsets_apriori = apriori(df, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# 提取关联规则\n",
    "# 使用association_rules函数从频繁项集中提取关联规则，设置最小置信度为0.5\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# 筛选高提升度规则\n",
    "# 筛选出提升度大于2.0的关联规则\n",
    "high_lift_rules_apriori = rules_apriori[rules_apriori[\"lift\"] > 2.0]\n",
    "\n",
    "# 打印频繁项集和关联规则\n",
    "print(\"Apriori频繁项集：\")\n",
    "print(frequent_itemsets_apriori)\n",
    "print(\"\\nApriori关联规则：\")\n",
    "print(rules_apriori)\n",
    "print(\"\\nApriori高提升度关联规则：\")\n",
    "print(high_lift_rules_apriori)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T09:39:24.507247700Z",
     "start_time": "2025-05-15T09:39:24.446408700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有关联规则可供绘制热力图，请调整参数或使用更低的 min_threshold。\n"
     ]
    }
   ],
   "source": [
    "# 可视化部分添加判断逻辑，防止空值导致报错\n",
    "if not rules_apriori.empty:\n",
    "    # 相关性分析 - 提升度热力图\n",
    "    rules_apriori_pivot = rules_apriori.pivot(index='antecedents', columns='consequents', values='lift')\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    # 创建图形\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # 绘制热力图\n",
    "    sns.heatmap(rules_apriori_pivot.fillna(0), annot=True, cmap='YlGnBu')\n",
    "    plt.title('Apriori Lift Heatmap')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"没有关联规则可供绘制热力图，请调整参数或使用更低的 min_threshold。\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T09:39:24.518465900Z",
     "start_time": "2025-05-15T09:39:24.502339200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2525)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2522)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2599)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:88)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 18\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# 创建新的 SparkSession\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFPGrowth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m新的 SparkSession 已创建。\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# 检查是否已有活跃的 SparkSession\u001B[39;00m\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    495\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[0;32m    496\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[1;32m--> 497\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[0;32m    499\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m    500\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:515\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    513\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 515\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:203\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[0;32m    201\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway\u001B[38;5;241m=\u001B[39mgateway, conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 203\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mappName\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparkHome\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpyFiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatchSize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjsc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprofiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[43mudf_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:296\u001B[0m, in \u001B[0;36mSparkContext._do_init\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvironment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    295\u001B[0m \u001B[38;5;66;03m# Create the Java SparkContext through Py4J\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc \u001B[38;5;241m=\u001B[39m jsc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[39;00m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;241m=\u001B[39m SparkConf(_jconf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39mconf())\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:421\u001B[0m, in \u001B[0;36mSparkContext._initialize_context\u001B[1;34m(self, jconf)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    418\u001B[0m \u001B[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 421\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mJavaSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjconf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1581\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1582\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1583\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1584\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1586\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1587\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1588\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1590\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1591\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2525)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2522)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2599)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:88)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import time  # 确保导入 time 模块\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 尝试获取当前活跃的 session\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is not None:\n",
    "        print(\"正在关闭已有的 SparkSession...\")\n",
    "        spark.stop()\n",
    "except Exception as e:\n",
    "    print(\"无活跃的 SparkSession，无需关闭。\")\n",
    "    pass\n",
    "\n",
    "# 创建新的 SparkSession\n",
    "spark = SparkSession.builder.appName(\"FPGrowth\").getOrCreate()\n",
    "print(\"新的 SparkSession 已创建。\")\n",
    "\n",
    "# 检查是否已有活跃的 SparkSession\n",
    "spark = SparkSession.builder.appName(\"FPGrowth\").getOrCreate()\n",
    "print(\"当前 SparkSession 状态：Active\" if spark.sparkContext.isStopped is False else \"已停止\")\n",
    "\n",
    "# 创建或获取 SparkSession\n",
    "spark = SparkSession.builder.appName(\"FPGrowth\").getOrCreate()\n",
    "\n",
    "# 假设 records 已经定义\n",
    "data_spark = spark.createDataFrame([(record,) for record in records], [\"items\"])\n",
    "\n",
    "# 初始化 FP-Growth 模型\n",
    "fp_growth = FPGrowth(itemsCol=\"items\", minSupport=0.05, minConfidence=0.5)\n",
    "\n",
    "# 记录开始时间\n",
    "start_time_fp = time.time()\n",
    "\n",
    "# 拟合模型\n",
    "model_fp = fp_growth.fit(data_spark)\n",
    "\n",
    "# 记录结束时间\n",
    "end_time_fp = time.time()\n",
    "\n",
    "# 提取结果\n",
    "frequent_itemsets_fp = model_fp.freqItemsets\n",
    "association_rules_fp = model_fp.associationRules\n",
    "\n",
    "# 打印结果\n",
    "print(\"FP - Growth频繁项集：\")\n",
    "frequent_itemsets_fp.show()\n",
    "print(\"\\nFP - Growth关联规则：\")\n",
    "association_rules_fp.show()\n",
    "\n",
    "# 性能对比分析\n",
    "start_time_ap = time.time()\n",
    "frequent_itemsets_apriori = apriori(df, min_support=0.05, use_colnames=True)\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=0.5)\n",
    "end_time_ap = time.time()\n",
    "\n",
    "print(f\"Apriori算法运行时间: {end_time_ap - start_time_ap} 秒\")\n",
    "print(f\"FP - Growth算法运行时间: {end_time_fp - start_time_fp} 秒\")  # 这里不再报错\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T09:41:07.612014300Z",
     "start_time": "2025-05-15T09:41:07.432377200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2525)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2522)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2599)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:88)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 38\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# 创建新的 SparkSession\u001B[39;00m\n\u001B[1;32m---> 38\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFPGrowth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSparkSession 已创建。\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# 转换为 Spark DataFrame\u001B[39;00m\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    495\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[0;32m    496\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[1;32m--> 497\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[0;32m    499\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m    500\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:515\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    513\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 515\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:203\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[0;32m    201\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway\u001B[38;5;241m=\u001B[39mgateway, conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 203\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mappName\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparkHome\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpyFiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatchSize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjsc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprofiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[43mudf_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:296\u001B[0m, in \u001B[0;36mSparkContext._do_init\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvironment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    295\u001B[0m \u001B[38;5;66;03m# Create the Java SparkContext through Py4J\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc \u001B[38;5;241m=\u001B[39m jsc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[39;00m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;241m=\u001B[39m SparkConf(_jconf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39mconf())\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\pyspark\\context.py:421\u001B[0m, in \u001B[0;36mSparkContext._initialize_context\u001B[1;34m(self, jconf)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    418\u001B[0m \u001B[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 421\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mJavaSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjconf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1581\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1582\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1583\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1584\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1586\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1587\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1588\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1590\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1591\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mD:\\python3\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\njava.lang.reflect.Constructor.newInstance(Unknown Source)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Unknown Source)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2525)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2522)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2599)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:88)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# 数据加载与预处理（Apriori）\n",
    "# -----------------------------\n",
    "data = pd.read_csv('Market_Basket_Optimisation.csv')\n",
    "records = data.apply(lambda row: row.dropna().astype(str).tolist(), axis=1).tolist()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# -----------------------------\n",
    "# Apriori 算法执行\n",
    "# -----------------------------\n",
    "frequent_itemsets_apriori = apriori(df, min_support=0.05, use_colnames=True)\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=0.3)\n",
    "high_lift_rules_apriori = rules_apriori[rules_apriori[\"lift\"] > 2.0]\n",
    "\n",
    "# -----------------------------\n",
    "# Spark 初始化（安全方式）\n",
    "# -----------------------------\n",
    "# 先检查是否有活跃的 SparkSession 并关闭\n",
    "try:\n",
    "    existing_spark = SparkSession.getActiveSession()\n",
    "    if existing_spark is not None:\n",
    "        print(\"检测到已有活跃的 SparkSession，正在关闭...\")\n",
    "        existing_spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 创建新的 SparkSession\n",
    "spark = SparkSession.builder.appName(\"FPGrowth\").getOrCreate()\n",
    "print(\"SparkSession 已创建。\")\n",
    "\n",
    "# 转换为 Spark DataFrame\n",
    "data_spark = spark.createDataFrame([(record,) for record in records], [\"items\"])\n",
    "\n",
    "# FP-Growth 模型训练\n",
    "fp_growth = FPGrowth(itemsCol=\"items\", minSupport=0.05, minConfidence=0.3)\n",
    "start_time_fp = time.time()\n",
    "model_fp = fp_growth.fit(data_spark)\n",
    "end_time_fp = time.time()\n",
    "\n",
    "# 提取频繁项集和关联规则\n",
    "frequent_itemsets_fp = model_fp.freqItemsets\n",
    "association_rules_fp = model_fp.associationRules\n",
    "\n",
    "# 打印结果\n",
    "print(\"FP - Growth频繁项集：\")\n",
    "frequent_itemsets_fp.show()\n",
    "print(\"\\nFP - Growth关联规则：\")\n",
    "association_rules_fp.show()\n",
    "\n",
    "# 性能对比\n",
    "start_time_ap = time.time()\n",
    "frequent_itemsets_apriori = apriori(df, min_support=0.05, use_colnames=True)\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=0.3)\n",
    "end_time_ap = time.time()\n",
    "\n",
    "print(f\"Apriori算法运行时间: {end_time_ap - start_time_ap} 秒\")\n",
    "print(f\"FP - Growth算法运行时间: {end_time_fp - start_time_fp} 秒\")\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T09:41:20.756187500Z",
     "start_time": "2025-05-15T09:41:18.613337400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
